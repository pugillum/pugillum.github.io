<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>🧱 Deploy a simple Databricks job with the Databricks SDK | Travis Dent</title>
<meta name=keywords content="data engineering,databricks"><meta name=description content="A simple script for deploying a Databricks job to run a simple Python script using the Databricks SDK"><meta name=author content><link rel=canonical href=https://pugillum.github.io/posts/2025_01_27_a_simple_databricks_job_deploy/><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://pugillum.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pugillum.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pugillum.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pugillum.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pugillum.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pugillum.github.io/posts/2025_01_27_a_simple_databricks_job_deploy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="🧱 Deploy a simple Databricks job with the Databricks SDK"><meta property="og:description" content="A simple script for deploying a Databricks job to run a simple Python script using the Databricks SDK"><meta property="og:type" content="article"><meta property="og:url" content="https://pugillum.github.io/posts/2025_01_27_a_simple_databricks_job_deploy/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-27T00:00:00+00:00"><meta property="og:site_name" content="Travis Dent"><meta name=twitter:card content="summary"><meta name=twitter:title content="🧱 Deploy a simple Databricks job with the Databricks SDK"><meta name=twitter:description content="A simple script for deploying a Databricks job to run a simple Python script using the Databricks SDK"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pugillum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"🧱 Deploy a simple Databricks job with the Databricks SDK","item":"https://pugillum.github.io/posts/2025_01_27_a_simple_databricks_job_deploy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"🧱 Deploy a simple Databricks job with the Databricks SDK","name":"🧱 Deploy a simple Databricks job with the Databricks SDK","description":"A simple script for deploying a Databricks job to run a simple Python script using the Databricks SDK","keywords":["data engineering","databricks"],"articleBody":"My use case is that I want to run a simple Python script as a Databricks job. So no Python wheels and definitely no notebooks. This could be useful for testing/debugging a smaller code sample or for one-time scripts for operational adjustments.\nThis was previously pretty simple with the use of the dbx CLI tool but work on that tool seems to have stopped and the recommendation is to use Databricks Asset Bundles\nDatabricks Asset Bundles may have it’s place but it felt too big for my use case and I decided to try out the new Databricks SDK for Python.\nThe solution is quite simple and can be seen below. I’ll break down what it does (and other considerations) in the rest of this post.\nfrom typing import Optional from databricks.sdk import WorkspaceClient from databricks.sdk.service.workspace import ImportFormat, Language from databricks.sdk.service.jobs import Task, SparkPythonTask from databricks.sdk.service.compute import ClusterSpec # Helper function to get the job ID by name def get_job_id(w: WorkspaceClient, job_name: str) -\u003e Optional[int]: for job in w.jobs.list(): if job_name == job.settings.name: return job.job_id return None def main(): w = WorkspaceClient() # Open the job.py file and read its content with open(\"job.py\", \"r\") as f: content = f.read() # Copy to a new location in the workspace w.workspace.mkdirs(\"/Shared/Scripts\") w.workspace.upload( path=\"/Shared/Scripts/job.py\", content=content, format=ImportFormat.RAW, language=Language.PYTHON, overwrite=True, ) # Specify the job cluster specification cluster_spec = ClusterSpec( num_workers=1, spark_version=\"13.3.x-cpu-scala2.12\", node_type_id=\"Standard_D4ds_v5\", ) # Specify the job task spark_python_task = SparkPythonTask( python_file=\"/Shared/Scripts/job.py\", parameters=[], ) task = Task( task_key=\"test\", new_cluster=cluster_spec, spark_python_task=spark_python_task, ) # Create or update the job job_name = \"Test Job via SDK\" job_id = get_job_id(w, job_name) if job_id: job_settings = w.jobs.get(job_id).settings job_settings.tasks = [task] w.jobs.reset(job_id, job_settings) else: w.jobs.create(name=job_name, tasks=[task]) if __name__ == \"__main__\": main() Setup To utilise this script to create a job in a Databricks workspace:\nInstall the Databricks CLI Ensure you have a Python environment Install the Python package databricks-sdkin your Python environment Copy the URL of your workspace (e.g. https://.cloud.databricks.com) Use the Databricks CLI to authenticate: Run databricks auth login For Databricks profile name: press Enter which will select the default profile For Databricks host paste the URL of your workspace You will be redirected to authenticate in your browser Once authentication has succeeded, you can check the contents of your .databrickscfg file ( default location for this file is in your ~ (your user home) folder on Unix, Linux, or macOS, or your %USERPROFILE% (your user home) folder on Windows) - there should be an entry matching your workspace URL Copy the Python script to a file (example main.py) and run it (example python main.py) You should see a new job in your workspace called Test Job via SDK Breaking it down The setup sequence:\nAuthenticate and setup workspace client This looks pretty simple:\nw = WorkspaceClient() When following the setup steps above, running this code will utilise the .databrickscfg file also known as a configuration profile file and will link to the workspace specified in the DEFAULT profile, utilising the stored authentication credentials.\nNote that the credentials expire in a much shorter time frame than would typically be the case with personal access tokens which may mean logging in more frequently. This is more secure!\nIt’s possible to define additional profiles when logging in, for example, you could use this line to define a dev profile:\ndatabricks auth login --profile dev and then use it for the workspace client:\nw = WorkspaceClient(profile=\"dev\") Additional authentication configurations are possible, for example with the use of environment variables. Read here for more info\nCopy job Python file to workspace In earlier times, Python files for jobs were copied to the Databricks File System (abbreviated as dbfs) but this is a deprecated pattern (see here). Instead we copy the job.py file to a workspace as a workspace file\nWe could have made use of Unity Catalog volumes for the Python file but decided to stick within the confines of a single workspace. For more on volumes, see here\nI should also note, the technique of reading in the entire Python file contents into memory with f.read() could be problematic if the file was really big (say above 100 MB) but I consider that a different kind of use case\nCreate or update job to run file Databricks jobs (or workflows in the UI) are composed of multiple tasks, hence the need to make use of the Task class. And then the SparkPythonTask is a separate dataclass for configuring the Task instance.\nUnfortunately there isn’t a direct way to override an existing job based purely on name so I had to retrieve a list of jobs and then get the job ID based on the name.\nThe job ID can be used with reset to configure the job. reset involves a full update of all job settings, partial updating of these settings can be done with update.\nCluster spec I use a cluster spec for defining a Job Cluster when running the job which defines a single node cluster.\nShould I had wanted to trigger repeated runs using an already running cluster I could have configured the code so:\n... cluster_id = \"\" spark_python_task = SparkPythonTask( python_file=\"/Shared/Scripts/job.py\", parameters=[], ) task = Task( task_key=\"test\", existing_cluster_id=cluster_id, spark_python_task=spark_python_task, ) ... Note, a working code sample along with a job.py and requirements.txt can be found here\n","wordCount":"882","inLanguage":"en","datePublished":"2025-01-27T00:00:00Z","dateModified":"2025-01-27T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://pugillum.github.io/posts/2025_01_27_a_simple_databricks_job_deploy/"},"publisher":{"@type":"Organization","name":"Travis Dent","logo":{"@type":"ImageObject","url":"https://pugillum.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pugillum.github.io/ accesskey=h title="  (Alt + H)"><img src=https://pugillum.github.io/avatar.png alt aria-label=logo height=35></a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://pugillum.github.io/archives/ title=archive><span>archive</span></a></li><li><a href=https://pugillum.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>🧱 Deploy a simple Databricks job with the Databricks SDK</h1><div class=post-meta><span title='2025-01-27 00:00:00 +0000 UTC'>January 27, 2025</span>&nbsp;·&nbsp;5 min</div></header><div class=post-content><p>My use case is that I want to run a simple Python script as a Databricks job. So no Python wheels and definitely no notebooks. This could be useful for testing/debugging a smaller code sample or for one-time scripts for operational adjustments.</p><p>This was previously pretty simple with the use of the <a href=https://github.com/databrickslabs/dbx>dbx CLI</a> tool but work on that tool seems to have stopped and the recommendation is to use <a href=https://docs.databricks.com/en/dev-tools/bundles/index.html>Databricks Asset Bundles</a></p><p>Databricks Asset Bundles may have it&rsquo;s place but it felt too big for my use case and I decided to try out the new <a href=https://docs.databricks.com/en/dev-tools/sdk-python.html>Databricks SDK for Python</a>.</p><p>The solution is quite simple and can be seen below. I&rsquo;ll break down what it does (and other considerations) in the rest of this post.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Optional
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> databricks.sdk <span style=color:#f92672>import</span> WorkspaceClient
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> databricks.sdk.service.workspace <span style=color:#f92672>import</span> ImportFormat, Language
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> databricks.sdk.service.jobs <span style=color:#f92672>import</span> Task, SparkPythonTask
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> databricks.sdk.service.compute <span style=color:#f92672>import</span> ClusterSpec
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Helper function to get the job ID by name</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_job_id</span>(w: WorkspaceClient, job_name: str) <span style=color:#f92672>-&gt;</span> Optional[int]:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> job <span style=color:#f92672>in</span> w<span style=color:#f92672>.</span>jobs<span style=color:#f92672>.</span>list():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> job_name <span style=color:#f92672>==</span> job<span style=color:#f92672>.</span>settings<span style=color:#f92672>.</span>name:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> job<span style=color:#f92672>.</span>job_id
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> WorkspaceClient()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Open the job.py file and read its content</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;job.py&#34;</span>, <span style=color:#e6db74>&#34;r&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        content <span style=color:#f92672>=</span> f<span style=color:#f92672>.</span>read()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Copy to a new location in the workspace</span>
</span></span><span style=display:flex><span>    w<span style=color:#f92672>.</span>workspace<span style=color:#f92672>.</span>mkdirs(<span style=color:#e6db74>&#34;/Shared/Scripts&#34;</span>)
</span></span><span style=display:flex><span>    w<span style=color:#f92672>.</span>workspace<span style=color:#f92672>.</span>upload(
</span></span><span style=display:flex><span>        path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/Shared/Scripts/job.py&#34;</span>,
</span></span><span style=display:flex><span>        content<span style=color:#f92672>=</span>content,
</span></span><span style=display:flex><span>        format<span style=color:#f92672>=</span>ImportFormat<span style=color:#f92672>.</span>RAW,
</span></span><span style=display:flex><span>        language<span style=color:#f92672>=</span>Language<span style=color:#f92672>.</span>PYTHON,
</span></span><span style=display:flex><span>        overwrite<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Specify the job cluster specification</span>
</span></span><span style=display:flex><span>    cluster_spec <span style=color:#f92672>=</span> ClusterSpec(
</span></span><span style=display:flex><span>        num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        spark_version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;13.3.x-cpu-scala2.12&#34;</span>,
</span></span><span style=display:flex><span>        node_type_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Standard_D4ds_v5&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Specify the job task</span>
</span></span><span style=display:flex><span>    spark_python_task <span style=color:#f92672>=</span> SparkPythonTask(
</span></span><span style=display:flex><span>        python_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/Shared/Scripts/job.py&#34;</span>,
</span></span><span style=display:flex><span>        parameters<span style=color:#f92672>=</span>[],
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    task <span style=color:#f92672>=</span> Task(
</span></span><span style=display:flex><span>        task_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;test&#34;</span>,
</span></span><span style=display:flex><span>        new_cluster<span style=color:#f92672>=</span>cluster_spec,
</span></span><span style=display:flex><span>        spark_python_task<span style=color:#f92672>=</span>spark_python_task,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create or update the job</span>
</span></span><span style=display:flex><span>    job_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Test Job via SDK&#34;</span>
</span></span><span style=display:flex><span>    job_id <span style=color:#f92672>=</span> get_job_id(w, job_name)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> job_id:
</span></span><span style=display:flex><span>        job_settings <span style=color:#f92672>=</span> w<span style=color:#f92672>.</span>jobs<span style=color:#f92672>.</span>get(job_id)<span style=color:#f92672>.</span>settings
</span></span><span style=display:flex><span>        job_settings<span style=color:#f92672>.</span>tasks <span style=color:#f92672>=</span> [task]
</span></span><span style=display:flex><span>        w<span style=color:#f92672>.</span>jobs<span style=color:#f92672>.</span>reset(job_id, job_settings)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        w<span style=color:#f92672>.</span>jobs<span style=color:#f92672>.</span>create(name<span style=color:#f92672>=</span>job_name, tasks<span style=color:#f92672>=</span>[task])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><h1 id=setup>Setup<a hidden class=anchor aria-hidden=true href=#setup>#</a></h1><p>To utilise this script to create a job in a Databricks workspace:</p><ol><li><a href=https://docs.databricks.com/en/dev-tools/cli/install.html>Install the Databricks CLI</a></li><li>Ensure you have a Python environment</li><li>Install the Python package <a href=https://pypi.org/project/databricks-sdk/>databricks-sdk</a>in your Python environment</li><li>Copy the URL of your workspace (e.g. <code>https://&lt;databricks-instance>.cloud.databricks.com</code>)</li><li>Use the Databricks CLI to authenticate:<ol><li>Run <code>databricks auth login</code></li><li>For <code>Databricks profile name:</code> press <code>Enter</code> which will select the default profile</li><li>For <code>Databricks host</code> paste the URL of your workspace</li><li>You will be redirected to authenticate in your browser</li><li>Once authentication has succeeded, you can check the contents of your <code>.databrickscfg</code> file ( default location for this file is in your <code>~</code> (your user home) folder on Unix, Linux, or macOS, or your <code>%USERPROFILE%</code> (your user home) folder on Windows) - there should be an entry matching your workspace URL</li></ol></li><li>Copy the Python script to a file (example <code>main.py</code>) and run it (example <code>python main.py</code>)</li><li>You should see a new job in your workspace called <code>Test Job via SDK</code></li></ol><h1 id=breaking-it-down>Breaking it down<a hidden class=anchor aria-hidden=true href=#breaking-it-down>#</a></h1><p>The setup sequence:</p><p><img loading=lazy src=./images/image-20250127151330136.png alt=image-20250127151330136></p><h1 id=authenticate-and-setup-workspace-client>Authenticate and setup workspace client<a hidden class=anchor aria-hidden=true href=#authenticate-and-setup-workspace-client>#</a></h1><p>This looks pretty simple:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>w <span style=color:#f92672>=</span> WorkspaceClient()
</span></span></code></pre></div><p>When following the setup steps above, running this code will utilise the <code>.databrickscfg</code> file also known as a <em>configuration profile file</em> and will link to the workspace specified in the <code>DEFAULT</code> profile, utilising the stored authentication credentials.</p><blockquote><p>Note that the credentials expire in a much shorter time frame than would typically be the case with personal access tokens which may mean logging in more frequently. This is more secure!</p></blockquote><p>It&rsquo;s possible to define additional profiles when logging in, for example, you could use this line to define a <em>dev</em> profile:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>databricks auth login --profile dev
</span></span></code></pre></div><p>and then use it for the workspace client:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>w <span style=color:#f92672>=</span> WorkspaceClient(profile<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;dev&#34;</span>)
</span></span></code></pre></div><p>Additional authentication configurations are possible, for example with the use of environment variables. Read <a href=https://databricks-sdk-py.readthedocs.io/en/latest/authentication.html>here</a> for more info</p><h1 id=copy-job-python-file-to-workspace>Copy job Python file to workspace<a hidden class=anchor aria-hidden=true href=#copy-job-python-file-to-workspace>#</a></h1><p>In earlier times, Python files for jobs were copied to the Databricks File System (abbreviated as <code>dbfs</code>) but this is a <em>deprecated pattern</em> (see <a href=https://docs.databricks.com/en/dbfs/index.html>here</a>). Instead we copy the <code>job.py</code> file to a workspace as a <a href=https://docs.databricks.com/en/files/workspace.html>workspace file</a></p><p>We could have made use of Unity Catalog volumes for the Python file but decided to stick within the confines of a single workspace. For more on volumes, see <a href=https://docs.databricks.com/en/files/index.html#work-with-files-in-unity-catalog-volumes>here</a></p><blockquote><p>I should also note, the technique of reading in the entire Python file contents into memory with <code>f.read()</code> could be problematic if the file was really big (say above 100 MB) but I consider that a different kind of use case</p></blockquote><h1 id=create-or-update-job-to-run-file>Create or update job to run file<a hidden class=anchor aria-hidden=true href=#create-or-update-job-to-run-file>#</a></h1><p>Databricks jobs (or workflows in the UI) are composed of multiple tasks, hence the need to make use of the <code>Task</code> class. And then the <code>SparkPythonTask</code> is a separate <a href=https://docs.python.org/3/library/dataclasses.html>dataclass</a> for configuring the <code>Task</code> instance.</p><p>Unfortunately there isn&rsquo;t a direct way to override an existing job based purely on name so I had to retrieve a list of jobs and then get the job ID based on the name.</p><p>The job ID can be used with <code>reset</code> to configure the job. <code>reset</code> involves a full update of all job settings, partial updating of these settings can be done with <code>update</code>.</p><h2 id=cluster-spec>Cluster spec<a hidden class=anchor aria-hidden=true href=#cluster-spec>#</a></h2><p>I use a cluster spec for defining a Job Cluster when running the job which defines a single node cluster.</p><p>Should I had wanted to trigger repeated runs using an already running cluster I could have configured the code so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    cluster_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&lt;id of all-purpose cluster&gt;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    spark_python_task <span style=color:#f92672>=</span> SparkPythonTask(
</span></span><span style=display:flex><span>        python_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/Shared/Scripts/job.py&#34;</span>,
</span></span><span style=display:flex><span>        parameters<span style=color:#f92672>=</span>[],
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    task <span style=color:#f92672>=</span> Task(
</span></span><span style=display:flex><span>        task_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;test&#34;</span>,
</span></span><span style=display:flex><span>        existing_cluster_id<span style=color:#f92672>=</span>cluster_id,
</span></span><span style=display:flex><span>        spark_python_task<span style=color:#f92672>=</span>spark_python_task,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span></code></pre></div><p>Note, a working code sample along with a <code>job.py</code> and <code>requirements.txt</code> can be found <a href=http://github.com/pugillum/blog_code_samples/blob/main/2025_01_deploy_a_simple_databricks_job/main.py>here</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://pugillum.github.io/tags/data-engineering/>Data Engineering</a></li><li><a href=https://pugillum.github.io/tags/databricks/>Databricks</a></li></ul><nav class=paginav><a class=next href=https://pugillum.github.io/posts/2021_11_28_drawio_in_markdown/><span class=title>Next »</span><br><span>✍️ All you need for your documentation is VS Code, Markdown and draw.io</span></a></nav></footer><script src=https://utteranc.es/client.js repo=pugillum/pugillum.github.io issue-term=pathname label=blog theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://pugillum.github.io/>Travis Dent</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>